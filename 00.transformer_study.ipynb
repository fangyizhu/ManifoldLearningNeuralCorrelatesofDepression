{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-14T17:08:13.596768Z",
     "start_time": "2025-08-14T17:07:57.616697Z"
    }
   },
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Users\\rhodo\\OneDrive\\Documents\\GitHub\\.venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "F:\\Users\\rhodo\\OneDrive\\Documents\\GitHub\\.venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "F:\\Users\\rhodo\\OneDrive\\Documents\\GitHub\\.venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "F:\\Users\\rhodo\\OneDrive\\Documents\\GitHub\\.venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "F:\\Users\\rhodo\\OneDrive\\Documents\\GitHub\\.venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "F:\\Users\\rhodo\\OneDrive\\Documents\\GitHub\\.venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "F:\\Users\\rhodo\\OneDrive\\Documents\\GitHub\\.venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "F:\\Users\\rhodo\\OneDrive\\Documents\\GitHub\\.venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "F:\\Users\\rhodo\\OneDrive\\Documents\\GitHub\\.venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "F:\\Users\\rhodo\\OneDrive\\Documents\\GitHub\\.venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "F:\\Users\\rhodo\\OneDrive\\Documents\\GitHub\\.venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "F:\\Users\\rhodo\\OneDrive\\Documents\\GitHub\\.venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "F:\\Users\\rhodo\\OneDrive\\Documents\\GitHub\\.venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "F:\\Users\\rhodo\\OneDrive\\Documents\\GitHub\\.venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "F:\\Users\\rhodo\\OneDrive\\Documents\\GitHub\\.venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "F:\\Users\\rhodo\\OneDrive\\Documents\\GitHub\\.venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "F:\\Users\\rhodo\\OneDrive\\Documents\\GitHub\\.venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "F:\\Users\\rhodo\\OneDrive\\Documents\\GitHub\\.venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "F:\\Users\\rhodo\\OneDrive\\Documents\\GitHub\\.venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "F:\\Users\\rhodo\\OneDrive\\Documents\\GitHub\\.venv\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T17:08:13.607185Z",
     "start_time": "2025-08-14T17:08:13.604171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MODEL = \"google/gemma-2b\"\n",
    "DEVICE = \"cuda:0\" # run on my gpu"
   ],
   "id": "22a605644289a2b2",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T17:08:14.775602Z",
     "start_time": "2025-08-14T17:08:13.621140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# A study of Tokenizers: https://learn.deeplearning.ai/courses/how-transformer-llms-work/lesson/e34gz/tokenizers\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "emotions = [\"happy\", \"sad\", \"anxious\", \"calm\", \"depressed\", \"elated\"]\n",
    "emotion_tokens = [tokenizer(emo).input_ids for emo in emotions]\n",
    "print(emotion_tokens)"
   ],
   "id": "6fad7857fb55e977",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 11896], [2, 37968], [2, 481, 24192], [2, 116051], [2, 3243, 3734], [2, 521, 840]]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T17:08:14.883860Z",
     "start_time": "2025-08-14T17:08:14.877038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DSM_DEPRESSION = \"Depression is a mood disorder that causes a persistent feeling of sadness and loss of interest.\"\n",
    "print(DSM_DEPRESSION)\n",
    "print(tokenizer(DSM_DEPRESSION))\n",
    "print(\"Depression\")\n",
    "print(tokenizer(\"Depression\"))\n",
    "print(\"depression\")\n",
    "print(tokenizer(\"depression\"))\n",
    "print(\"Depressed\")\n",
    "print(tokenizer(\"Depressed\"))\n",
    "print(\"Depress\")\n",
    "print(tokenizer(\"Depress\"))\n",
    "print(\"Dep\")\n",
    "print(tokenizer(\"Dep\"))\n",
    "print(\"pression\")\n",
    "print(tokenizer(\"pression\"))\n",
    "print(\"dep\")\n",
    "print(tokenizer(\"dep\"))\n",
    "print(\"Depp\")\n",
    "print(tokenizer(\"Depp\"))\n",
    "print(\"press\")\n",
    "print(tokenizer(\"press\"))\n",
    "print(\"pressed\")\n",
    "print(tokenizer(\"pressed\"))\n",
    "print(\"pression\")\n",
    "print(tokenizer(\"pression\"))"
   ],
   "id": "179e28818cc5e152",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depression is a mood disorder that causes a persistent feeling of sadness and loss of interest.\n",
      "{'input_ids': [2, 116465, 603, 476, 18068, 24283, 674, 10792, 476, 36727, 7965, 576, 51863, 578, 4783, 576, 3273, 235265], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "Depression\n",
      "{'input_ids': [2, 116465], 'attention_mask': [1, 1]}\n",
      "depression\n",
      "{'input_ids': [2, 161067], 'attention_mask': [1, 1]}\n",
      "Depressed\n",
      "{'input_ids': [2, 5789, 3734], 'attention_mask': [1, 1, 1]}\n",
      "Depress\n",
      "{'input_ids': [2, 5789, 1054], 'attention_mask': [1, 1, 1]}\n",
      "Dep\n",
      "{'input_ids': [2, 5789], 'attention_mask': [1, 1]}\n",
      "pression\n",
      "{'input_ids': [2, 206753], 'attention_mask': [1, 1]}\n",
      "dep\n",
      "{'input_ids': [2, 3243], 'attention_mask': [1, 1]}\n",
      "Depp\n",
      "{'input_ids': [2, 1680, 658], 'attention_mask': [1, 1, 1]}\n",
      "press\n",
      "{'input_ids': [2, 11355], 'attention_mask': [1, 1]}\n",
      "pressed\n",
      "{'input_ids': [2, 49716], 'attention_mask': [1, 1]}\n",
      "pression\n",
      "{'input_ids': [2, 206753], 'attention_mask': [1, 1]}\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T17:08:14.915477Z",
     "start_time": "2025-08-14T17:08:14.909783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_tokens = [8000, 15210, 613, 4521, 799, 23406]\n",
    "print(test_tokens)\n",
    "print([tokenizer.decode(tok) for tok in test_tokens])\n",
    "# decode: token id -> text, encode: text -> token id, code ~= token"
   ],
   "id": "d29781f8f8e290f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8000, 15210, 613, 4521, 799, 23406]\n",
      "['Chapter', 'лай', ' wi', 'Hello', ' tr', ' sü']\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Note: In gpt-oss \"Depression\" and \"depression\" have different tokens. And they are both split into two tokens each.",
   "id": "7c07f349f1fdb9a1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T17:08:14.944546Z",
     "start_time": "2025-08-14T17:08:14.940220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# A list of colors in RGB for representing the tokens\n",
    "colors = [\n",
    "    '102;194;165', '252;141;98', '141;160;203',\n",
    "    '231;138;195', '166;216;84', '255;217;47'\n",
    "]\n",
    "\n",
    "def show_tokens(sentence: str, tokenizer_name: str):\n",
    "    \"\"\" Show the tokens each separated by a different color \"\"\"\n",
    "\n",
    "    # Load the tokenizer and tokenize the input\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    token_ids = tokenizer(sentence).input_ids\n",
    "\n",
    "    # Extract vocabulary length\n",
    "    print(f\"Vocab length: {len(tokenizer)}\")\n",
    "\n",
    "    # Print a colored list of tokens\n",
    "    for idx, t in enumerate(token_ids):\n",
    "        print(\n",
    "            f'\\x1b[0;30;48;2;{colors[idx % len(colors)]}m' +\n",
    "            tokenizer.decode(t) +\n",
    "            '\\x1b[0m',\n",
    "            end=' '\n",
    "        )"
   ],
   "id": "dd62806e920ec09e",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T17:08:16.041051Z",
     "start_time": "2025-08-14T17:08:14.969045Z"
    }
   },
   "cell_type": "code",
   "source": "show_tokens(DSM_DEPRESSION, MODEL)",
   "id": "c6fbf95aab797f36",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab length: 256000\n",
      "\u001B[0;30;48;2;102;194;165m<bos>\u001B[0m \u001B[0;30;48;2;252;141;98mDepression\u001B[0m \u001B[0;30;48;2;141;160;203m is\u001B[0m \u001B[0;30;48;2;231;138;195m a\u001B[0m \u001B[0;30;48;2;166;216;84m mood\u001B[0m \u001B[0;30;48;2;255;217;47m disorder\u001B[0m \u001B[0;30;48;2;102;194;165m that\u001B[0m \u001B[0;30;48;2;252;141;98m causes\u001B[0m \u001B[0;30;48;2;141;160;203m a\u001B[0m \u001B[0;30;48;2;231;138;195m persistent\u001B[0m \u001B[0;30;48;2;166;216;84m feeling\u001B[0m \u001B[0;30;48;2;255;217;47m of\u001B[0m \u001B[0;30;48;2;102;194;165m sadness\u001B[0m \u001B[0;30;48;2;252;141;98m and\u001B[0m \u001B[0;30;48;2;141;160;203m loss\u001B[0m \u001B[0;30;48;2;231;138;195m of\u001B[0m \u001B[0;30;48;2;166;216;84m interest\u001B[0m \u001B[0;30;48;2;255;217;47m.\u001B[0m "
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Observation: gpt-oss models share the same tokenizer between two model sizes.",
   "id": "9c794a871c90872f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T17:08:32.418398Z",
     "start_time": "2025-08-14T17:08:16.116439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# A study of Transformers: https://learn.deeplearning.ai/courses/how-transformer-llms-work/lesson/m3nid/model-example\n",
    "torch.cuda.empty_cache()\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL,\n",
    "    device_map=\"cuda:0\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")"
   ],
   "id": "b52ccaab2842b32f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2349a84757f14d459cf0082892d11ac2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T17:08:32.504319Z",
     "start_time": "2025-08-14T17:08:32.499281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False, # False means to not include the prompt text in the returned text\n",
    "    max_new_tokens=50,\n",
    "    do_sample=False, # no randomness in the generated text\n",
    ")"
   ],
   "id": "9273aa1a1a618900",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T17:08:35.146323Z",
     "start_time": "2025-08-14T17:08:32.533246Z"
    }
   },
   "cell_type": "code",
   "source": "print(generator(DSM_DEPRESSION)[0]['generated_text'])",
   "id": "3de01c77e389b877",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " It can affect your thoughts, behavior, and physical health. Depression can be caused by a variety of factors, including genetics, brain chemistry, and life events.\n",
      "\n",
      "There are many different types of depression, and the symptoms can vary from person to person\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T17:08:35.235785Z",
     "start_time": "2025-08-14T17:08:35.230740Z"
    }
   },
   "cell_type": "code",
   "source": "model",
   "id": "6f6215974dac1359",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): GELUActivation()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "    (rotary_emb): GemmaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T17:08:35.268775Z",
     "start_time": "2025-08-14T17:08:35.265055Z"
    }
   },
   "cell_type": "code",
   "source": "model.model.embed_tokens",
   "id": "b410525b8f98316d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(256000, 2048, padding_idx=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T17:08:35.302949Z",
     "start_time": "2025-08-14T17:08:35.298087Z"
    }
   },
   "cell_type": "code",
   "source": "model.model # printing the stack of transformer blocks without the LM head component",
   "id": "40afbabb0bb2ad03",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaModel(\n",
       "  (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "  (layers): ModuleList(\n",
       "    (0-17): 18 x GemmaDecoderLayer(\n",
       "      (self_attn): GemmaAttention(\n",
       "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "        (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "      )\n",
       "      (mlp): GemmaMLP(\n",
       "        (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "        (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "        (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "        (act_fn): GELUActivation()\n",
       "      )\n",
       "      (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "      (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "    )\n",
       "  )\n",
       "  (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "  (rotary_emb): GemmaRotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T17:08:35.337092Z",
     "start_time": "2025-08-14T17:08:35.332517Z"
    }
   },
   "cell_type": "code",
   "source": "model.model.layers[0]",
   "id": "e09996c2aaa4d72b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaDecoderLayer(\n",
       "  (self_attn): GemmaAttention(\n",
       "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "    (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "    (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "  )\n",
       "  (mlp): GemmaMLP(\n",
       "    (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "    (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "    (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "    (act_fn): GELUActivation()\n",
       "  )\n",
       "  (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "  (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T17:08:35.374512Z",
     "start_time": "2025-08-14T17:08:35.362478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "print(DSM_DEPRESSION)\n",
    "input_ids = tokenizer(DSM_DEPRESSION, return_tensors=\"pt\").input_ids.to(DEVICE) # pt probably means pytorch\n",
    "input_ids"
   ],
   "id": "4e0d024fb0465bde",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depression is a mood disorder that causes a persistent feeling of sadness and loss of interest.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[     2, 116465,    603,    476,  18068,  24283,    674,  10792,    476,\n",
       "          36727,   7965,    576,  51863,    578,   4783,    576,   3273, 235265]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T17:08:35.466665Z",
     "start_time": "2025-08-14T17:08:35.404920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get the output of the model before the lm_head\n",
    "model_output = model.model(input_ids)"
   ],
   "id": "351a2bf8bfac464c",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T17:08:35.480139Z",
     "start_time": "2025-08-14T17:08:35.476075Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get the shape the output the model before the lm_head\n",
    "model_output[0].shape"
   ],
   "id": "56f7b54b4efe6401",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 18, 2048])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T17:08:35.508284Z",
     "start_time": "2025-08-14T17:08:35.503942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get the output of the lm_head\n",
    "lm_head_output = model.lm_head(model_output[0])\n",
    "lm_head_output.shape"
   ],
   "id": "1607a3a3e4545ec8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 18, 256000])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T17:08:35.536780Z",
     "start_time": "2025-08-14T17:08:35.530665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "token_id = lm_head_output[0,-1].argmax(-1)\n",
    "token_id"
   ],
   "id": "b3d14aaf4e442c4c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1165, device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T17:08:35.598320Z",
     "start_time": "2025-08-14T17:08:35.593242Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.decode(token_id)",
   "id": "70280dc79c1f045d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' It'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T17:08:48.358331Z",
     "start_time": "2025-08-14T17:08:48.350703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "named_layers = dict(model.named_modules())\n",
    "print(named_layers)"
   ],
   "id": "2a740ec20dd0d311",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': GemmaForCausalLM(\n",
      "  (model): GemmaModel(\n",
      "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-17): 18 x GemmaDecoderLayer(\n",
      "        (self_attn): GemmaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): GemmaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "          (act_fn): GELUActivation()\n",
      "        )\n",
      "        (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "        (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "    (rotary_emb): GemmaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
      "), 'model': GemmaModel(\n",
      "  (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
      "  (layers): ModuleList(\n",
      "    (0-17): 18 x GemmaDecoderLayer(\n",
      "      (self_attn): GemmaAttention(\n",
      "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      )\n",
      "      (mlp): GemmaMLP(\n",
      "        (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "        (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "        (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "        (act_fn): GELUActivation()\n",
      "      )\n",
      "      (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "      (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "    )\n",
      "  )\n",
      "  (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "  (rotary_emb): GemmaRotaryEmbedding()\n",
      "), 'model.embed_tokens': Embedding(256000, 2048, padding_idx=0), 'model.layers': ModuleList(\n",
      "  (0-17): 18 x GemmaDecoderLayer(\n",
      "    (self_attn): GemmaAttention(\n",
      "      (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "      (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "      (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "      (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    )\n",
      "    (mlp): GemmaMLP(\n",
      "      (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "      (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "      (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "      (act_fn): GELUActivation()\n",
      "    )\n",
      "    (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "    (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "  )\n",
      "), 'model.layers.0': GemmaDecoderLayer(\n",
      "  (self_attn): GemmaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (mlp): GemmaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "    (act_fn): GELUActivation()\n",
      "  )\n",
      "  (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "), 'model.layers.0.self_attn': GemmaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "), 'model.layers.0.self_attn.q_proj': Linear(in_features=2048, out_features=2048, bias=False), 'model.layers.0.self_attn.k_proj': Linear(in_features=2048, out_features=256, bias=False), 'model.layers.0.self_attn.v_proj': Linear(in_features=2048, out_features=256, bias=False), 'model.layers.0.self_attn.o_proj': Linear(in_features=2048, out_features=2048, bias=False), 'model.layers.0.mlp': GemmaMLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "  (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "  (act_fn): GELUActivation()\n",
      "), 'model.layers.0.mlp.gate_proj': Linear(in_features=2048, out_features=16384, bias=False), 'model.layers.0.mlp.up_proj': Linear(in_features=2048, out_features=16384, bias=False), 'model.layers.0.mlp.down_proj': Linear(in_features=16384, out_features=2048, bias=False), 'model.layers.0.mlp.act_fn': GELUActivation(), 'model.layers.0.input_layernorm': GemmaRMSNorm((2048,), eps=1e-06), 'model.layers.0.post_attention_layernorm': GemmaRMSNorm((2048,), eps=1e-06), 'model.layers.1': GemmaDecoderLayer(\n",
      "  (self_attn): GemmaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (mlp): GemmaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "    (act_fn): GELUActivation()\n",
      "  )\n",
      "  (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "), 'model.layers.1.self_attn': GemmaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "), 'model.layers.1.self_attn.q_proj': Linear(in_features=2048, out_features=2048, bias=False), 'model.layers.1.self_attn.k_proj': Linear(in_features=2048, out_features=256, bias=False), 'model.layers.1.self_attn.v_proj': Linear(in_features=2048, out_features=256, bias=False), 'model.layers.1.self_attn.o_proj': Linear(in_features=2048, out_features=2048, bias=False), 'model.layers.1.mlp': GemmaMLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "  (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "  (act_fn): GELUActivation()\n",
      "), 'model.layers.1.mlp.gate_proj': Linear(in_features=2048, out_features=16384, bias=False), 'model.layers.1.mlp.up_proj': Linear(in_features=2048, out_features=16384, bias=False), 'model.layers.1.mlp.down_proj': Linear(in_features=16384, out_features=2048, bias=False), 'model.layers.1.mlp.act_fn': GELUActivation(), 'model.layers.1.input_layernorm': GemmaRMSNorm((2048,), eps=1e-06), 'model.layers.1.post_attention_layernorm': GemmaRMSNorm((2048,), eps=1e-06), 'model.layers.2': GemmaDecoderLayer(\n",
      "  (self_attn): GemmaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (mlp): GemmaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "    (act_fn): GELUActivation()\n",
      "  )\n",
      "  (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "), 'model.layers.2.self_attn': GemmaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "), 'model.layers.2.self_attn.q_proj': Linear(in_features=2048, out_features=2048, bias=False), 'model.layers.2.self_attn.k_proj': Linear(in_features=2048, out_features=256, bias=False), 'model.layers.2.self_attn.v_proj': Linear(in_features=2048, out_features=256, bias=False), 'model.layers.2.self_attn.o_proj': Linear(in_features=2048, out_features=2048, bias=False), 'model.layers.2.mlp': GemmaMLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "  (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "  (act_fn): GELUActivation()\n",
      "), 'model.layers.2.mlp.gate_proj': Linear(in_features=2048, out_features=16384, bias=False), 'model.layers.2.mlp.up_proj': Linear(in_features=2048, out_features=16384, bias=False), 'model.layers.2.mlp.down_proj': Linear(in_features=16384, out_features=2048, bias=False), 'model.layers.2.mlp.act_fn': GELUActivation(), 'model.layers.2.input_layernorm': GemmaRMSNorm((2048,), eps=1e-06), 'model.layers.2.post_attention_layernorm': GemmaRMSNorm((2048,), eps=1e-06), 'model.layers.3': GemmaDecoderLayer(\n",
      "  (self_attn): GemmaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (mlp): GemmaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "    (act_fn): GELUActivation()\n",
      "  )\n",
      "  (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "), 'model.layers.3.self_attn': GemmaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "), 'model.layers.3.self_attn.q_proj': Linear(in_features=2048, out_features=2048, bias=False), 'model.layers.3.self_attn.k_proj': Linear(in_features=2048, out_features=256, bias=False), 'model.layers.3.self_attn.v_proj': Linear(in_features=2048, out_features=256, bias=False), 'model.layers.3.self_attn.o_proj': Linear(in_features=2048, out_features=2048, bias=False), 'model.layers.3.mlp': GemmaMLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "  (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "  (act_fn): GELUActivation()\n",
      "), 'model.layers.3.mlp.gate_proj': Linear(in_features=2048, out_features=16384, bias=False), 'model.layers.3.mlp.up_proj': Linear(in_features=2048, out_features=16384, bias=False), 'model.layers.3.mlp.down_proj': Linear(in_features=16384, out_features=2048, bias=False), 'model.layers.3.mlp.act_fn': GELUActivation(), 'model.layers.3.input_layernorm': GemmaRMSNorm((2048,), eps=1e-06), 'model.layers.3.post_attention_layernorm': GemmaRMSNorm((2048,), eps=1e-06), 'model.layers.4': GemmaDecoderLayer(\n",
      "  (self_attn): GemmaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (mlp): GemmaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "    (act_fn): GELUActivation()\n",
      "  )\n",
      "  (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "), 'model.layers.4.self_attn': GemmaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "), 'model.layers.4.self_attn.q_proj': Linear(in_features=2048, out_features=2048, bias=False), 'model.layers.4.self_attn.k_proj': Linear(in_features=2048, out_features=256, bias=False), 'model.layers.4.self_attn.v_proj': Linear(in_features=2048, out_features=256, bias=False), 'model.layers.4.self_attn.o_proj': Linear(in_features=2048, out_features=2048, bias=False), 'model.layers.4.mlp': GemmaMLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "  (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "  (act_fn): GELUActivation()\n",
      "), 'model.layers.4.mlp.gate_proj': Linear(in_features=2048, out_features=16384, bias=False), 'model.layers.4.mlp.up_proj': Linear(in_features=2048, out_features=16384, bias=False), 'model.layers.4.mlp.down_proj': Linear(in_features=16384, out_features=2048, bias=False), 'model.layers.4.mlp.act_fn': GELUActivation(), 'model.layers.4.input_layernorm': GemmaRMSNorm((2048,), eps=1e-06), 'model.layers.4.post_attention_layernorm': GemmaRMSNorm((2048,), eps=1e-06), 'model.layers.5': GemmaDecoderLayer(\n",
      "  (self_attn): GemmaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (mlp): GemmaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "    (act_fn): GELUActivation()\n",
      "  )\n",
      "  (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "), 'model.layers.5.self_attn': GemmaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "), 'model.layers.5.self_attn.q_proj': Linear(in_features=2048, out_features=2048, bias=False), 'model.layers.5.self_attn.k_proj': Linear(in_features=2048, out_features=256, bias=False), 'model.layers.5.self_attn.v_proj': Linear(in_features=2048, out_features=256, bias=False), 'model.layers.5.self_attn.o_proj': Linear(in_features=2048, out_features=2048, bias=False), 'model.layers.5.mlp': GemmaMLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "  (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "  (act_fn): GELUActivation()\n",
      "), 'model.layers.5.mlp.gate_proj': Linear(in_features=2048, out_features=16384, bias=False), 'model.layers.5.mlp.up_proj': Linear(in_features=2048, out_features=16384, bias=False), 'model.layers.5.mlp.down_proj': Linear(in_features=16384, out_features=2048, bias=False), 'model.layers.5.mlp.act_fn': GELUActivation(), 'model.layers.5.input_layernorm': GemmaRMSNorm((2048,), eps=1e-06), 'model.layers.5.post_attention_layernorm': GemmaRMSNorm((2048,), eps=1e-06), 'model.layers.6': GemmaDecoderLayer(\n",
      "  (self_attn): GemmaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (mlp): GemmaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "    (act_fn): GELUActivation()\n",
      "  )\n",
      "  (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "), 'model.layers.6.self_attn': GemmaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "), 'model.layers.6.self_attn.q_proj': Linear(in_features=2048, out_features=2048, bias=False), 'model.layers.6.self_attn.k_proj': Linear(in_features=2048, out_features=256, bias=False), 'model.layers.6.self_attn.v_proj': Linear(in_features=2048, out_features=256, bias=False), 'model.layers.6.self_attn.o_proj': Linear(in_features=2048, out_features=2048, bias=False), 'model.layers.6.mlp': GemmaMLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "  (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "  (act_fn): GELUActivation()\n",
      "), 'model.layers.6.mlp.gate_proj': Linear(in_features=2048, out_features=16384, bias=False), 'model.layers.6.mlp.up_proj': Linear(in_features=2048, out_features=16384, bias=False), 'model.layers.6.mlp.down_proj': Linear(in_features=16384, out_features=2048, bias=False), 'model.layers.6.mlp.act_fn': GELUActivation(), 'model.layers.6.input_layernorm': GemmaRMSNorm((2048,), eps=1e-06), 'model.layers.6.post_attention_layernorm': GemmaRMSNorm((2048,), eps=1e-06), 'model.layers.7': GemmaDecoderLayer(\n",
      "  (self_attn): GemmaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (mlp): GemmaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "    (act_fn): GELUActivation()\n",
      "  )\n",
      "  (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "), 'model.layers.7.self_attn': GemmaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "), 'model.layers.7.self_attn.q_proj': Linear(in_features=2048, out_features=2048, bias=False), 'model.layers.7.self_attn.k_proj': Linear(in_features=2048, out_features=256, bias=False), 'model.layers.7.self_attn.v_proj': Linear(in_features=2048, out_features=256, bias=False), 'model.layers.7.self_attn.o_proj': Linear(in_features=2048, out_features=2048, bias=False), 'model.layers.7.mlp': GemmaMLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "  (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "  (act_fn): GELUActivation()\n",
      "), 'model.layers.7.mlp.gate_proj': Linear(in_features=2048, out_features=16384, bias=False), 'model.layers.7.mlp.up_proj': Linear(in_features=2048, out_features=16384, bias=False), 'model.layers.7.mlp.down_proj': Linear(in_features=16384, out_features=2048, bias=False), 'model.layers.7.mlp.act_fn': GELUActivation(), 'model.layers.7.input_layernorm': GemmaRMSNorm((2048,), eps=1e-06), 'model.layers.7.post_attention_layernorm': GemmaRMSNorm((2048,), eps=1e-06), 'model.layers.8': GemmaDecoderLayer(\n",
      "  (self_attn): GemmaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (mlp): GemmaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "    (act_fn): GELUActivation()\n",
      "  )\n",
      "  (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "), 'model.layers.8.self_attn': GemmaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "), 'model.layers.8.self_attn.q_proj': Linear(in_features=2048, out_features=2048, bias=False), 'model.layers.8.self_attn.k_proj': Linear(in_features=2048, out_features=256, bias=False), 'model.layers.8.self_attn.v_proj': Linear(in_features=2048, out_features=256, bias=False), 'model.layers.8.self_attn.o_proj': Linear(in_features=2048, out_features=2048, bias=False), 'model.layers.8.mlp': GemmaMLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "  (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "  (act_fn): GELUActivation()\n",
      "), 'model.layers.8.mlp.gate_proj': Linear(in_features=2048, out_features=16384, bias=False), 'model.layers.8.mlp.up_proj': Linear(in_features=2048, out_features=16384, bias=False), 'model.layers.8.mlp.down_proj': Linear(in_features=16384, out_features=2048, bias=False), 'model.layers.8.mlp.act_fn': GELUActivation(), 'model.layers.8.input_layernorm': GemmaRMSNorm((2048,), eps=1e-06), 'model.layers.8.post_attention_layernorm': GemmaRMSNorm((2048,), eps=1e-06), 'model.layers.9': GemmaDecoderLayer(\n",
      "  (self_attn): GemmaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (mlp): GemmaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "    (act_fn): GELUActivation()\n",
      "  )\n",
      "  (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "), 'model.layers.9.self_attn': GemmaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "), 'model.layers.9.self_attn.q_proj': Linear(in_features=2048, out_features=2048, bias=False), 'model.layers.9.self_attn.k_proj': Linear(in_features=2048, out_features=256, bias=False), 'model.layers.9.self_attn.v_proj': Linear(in_features=2048, out_features=256, bias=False), 'model.layers.9.self_attn.o_proj': Linear(in_features=2048, out_features=2048, bias=False), 'model.layers.9.mlp': GemmaMLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "  (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "  (act_fn): GELUActivation()\n",
      "), 'model.layers.9.mlp.gate_proj': Linear(in_features=2048, out_features=16384, bias=False), 'model.layers.9.mlp.up_proj': Linear(in_features=2048, out_features=16384, bias=False), 'model.layers.9.mlp.down_proj': Linear(in_features=16384, out_features=2048, bias=False), 'model.layers.9.mlp.act_fn': GELUActivation(), 'model.layers.9.input_layernorm': GemmaRMSNorm((2048,), eps=1e-06), 'model.layers.9.post_attention_layernorm': GemmaRMSNorm((2048,), eps=1e-06), 'model.layers.10': GemmaDecoderLayer(\n",
      "  (self_attn): GemmaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (mlp): GemmaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "    (act_fn): GELUActivation()\n",
      "  )\n",
      "  (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "), 'model.layers.10.self_attn': GemmaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "), 'model.layers.10.self_attn.q_proj': Linear(in_features=2048, out_features=2048, bias=False), 'model.layers.10.self_attn.k_proj': Linear(in_features=2048, out_features=256, bias=False), 'model.layers.10.self_attn.v_proj': Linear(in_features=2048, out_features=256, bias=False), 'model.layers.10.self_attn.o_proj': Linear(in_features=2048, out_features=2048, bias=False), 'model.layers.10.mlp': GemmaMLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "  (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "  (act_fn): GELUActivation()\n",
      "), 'model.layers.10.mlp.gate_proj': Linear(in_features=2048, out_features=16384, bias=False), 'model.layers.10.mlp.up_proj': Linear(in_features=2048, out_features=16384, bias=False), 'model.layers.10.mlp.down_proj': Linear(in_features=16384, out_features=2048, bias=False), 'model.layers.10.mlp.act_fn': GELUActivation(), 'model.layers.10.input_layernorm': GemmaRMSNorm((2048,), eps=1e-06), 'model.layers.10.post_attention_layernorm': GemmaRMSNorm((2048,), eps=1e-06), 'model.layers.11': GemmaDecoderLayer(\n",
      "  (self_attn): GemmaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (mlp): GemmaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "    (act_fn): GELUActivation()\n",
      "  )\n",
      "  (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "), 'model.layers.11.self_attn': GemmaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "), 'model.layers.11.self_attn.q_proj': Linear(in_features=2048, out_features=2048, bias=False), 'model.layers.11.self_attn.k_proj': Linear(in_features=2048, out_features=256, bias=False), 'model.layers.11.self_attn.v_proj': Linear(in_features=2048, out_features=256, bias=False), 'model.layers.11.self_attn.o_proj': Linear(in_features=2048, out_features=2048, bias=False), 'model.layers.11.mlp': GemmaMLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "  (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "  (act_fn): GELUActivation()\n",
      "), 'model.layers.11.mlp.gate_proj': Linear(in_features=2048, out_features=16384, bias=False), 'model.layers.11.mlp.up_proj': Linear(in_features=2048, out_features=16384, bias=False), 'model.layers.11.mlp.down_proj': Linear(in_features=16384, out_features=2048, bias=False), 'model.layers.11.mlp.act_fn': GELUActivation(), 'model.layers.11.input_layernorm': GemmaRMSNorm((2048,), eps=1e-06), 'model.layers.11.post_attention_layernorm': GemmaRMSNorm((2048,), eps=1e-06), 'model.layers.12': GemmaDecoderLayer(\n",
      "  (self_attn): GemmaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (mlp): GemmaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "    (act_fn): GELUActivation()\n",
      "  )\n",
      "  (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "), 'model.layers.12.self_attn': GemmaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "), 'model.layers.12.self_attn.q_proj': Linear(in_features=2048, out_features=2048, bias=False), 'model.layers.12.self_attn.k_proj': Linear(in_features=2048, out_features=256, bias=False), 'model.layers.12.self_attn.v_proj': Linear(in_features=2048, out_features=256, bias=False), 'model.layers.12.self_attn.o_proj': Linear(in_features=2048, out_features=2048, bias=False), 'model.layers.12.mlp': GemmaMLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "  (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "  (act_fn): GELUActivation()\n",
      "), 'model.layers.12.mlp.gate_proj': Linear(in_features=2048, out_features=16384, bias=False), 'model.layers.12.mlp.up_proj': Linear(in_features=2048, out_features=16384, bias=False), 'model.layers.12.mlp.down_proj': Linear(in_features=16384, out_features=2048, bias=False), 'model.layers.12.mlp.act_fn': GELUActivation(), 'model.layers.12.input_layernorm': GemmaRMSNorm((2048,), eps=1e-06), 'model.layers.12.post_attention_layernorm': GemmaRMSNorm((2048,), eps=1e-06), 'model.layers.13': GemmaDecoderLayer(\n",
      "  (self_attn): GemmaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (mlp): GemmaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "    (act_fn): GELUActivation()\n",
      "  )\n",
      "  (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "), 'model.layers.13.self_attn': GemmaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "), 'model.layers.13.self_attn.q_proj': Linear(in_features=2048, out_features=2048, bias=False), 'model.layers.13.self_attn.k_proj': Linear(in_features=2048, out_features=256, bias=False), 'model.layers.13.self_attn.v_proj': Linear(in_features=2048, out_features=256, bias=False), 'model.layers.13.self_attn.o_proj': Linear(in_features=2048, out_features=2048, bias=False), 'model.layers.13.mlp': GemmaMLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "  (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "  (act_fn): GELUActivation()\n",
      "), 'model.layers.13.mlp.gate_proj': Linear(in_features=2048, out_features=16384, bias=False), 'model.layers.13.mlp.up_proj': Linear(in_features=2048, out_features=16384, bias=False), 'model.layers.13.mlp.down_proj': Linear(in_features=16384, out_features=2048, bias=False), 'model.layers.13.mlp.act_fn': GELUActivation(), 'model.layers.13.input_layernorm': GemmaRMSNorm((2048,), eps=1e-06), 'model.layers.13.post_attention_layernorm': GemmaRMSNorm((2048,), eps=1e-06), 'model.layers.14': GemmaDecoderLayer(\n",
      "  (self_attn): GemmaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (mlp): GemmaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "    (act_fn): GELUActivation()\n",
      "  )\n",
      "  (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "), 'model.layers.14.self_attn': GemmaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "), 'model.layers.14.self_attn.q_proj': Linear(in_features=2048, out_features=2048, bias=False), 'model.layers.14.self_attn.k_proj': Linear(in_features=2048, out_features=256, bias=False), 'model.layers.14.self_attn.v_proj': Linear(in_features=2048, out_features=256, bias=False), 'model.layers.14.self_attn.o_proj': Linear(in_features=2048, out_features=2048, bias=False), 'model.layers.14.mlp': GemmaMLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "  (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "  (act_fn): GELUActivation()\n",
      "), 'model.layers.14.mlp.gate_proj': Linear(in_features=2048, out_features=16384, bias=False), 'model.layers.14.mlp.up_proj': Linear(in_features=2048, out_features=16384, bias=False), 'model.layers.14.mlp.down_proj': Linear(in_features=16384, out_features=2048, bias=False), 'model.layers.14.mlp.act_fn': GELUActivation(), 'model.layers.14.input_layernorm': GemmaRMSNorm((2048,), eps=1e-06), 'model.layers.14.post_attention_layernorm': GemmaRMSNorm((2048,), eps=1e-06), 'model.layers.15': GemmaDecoderLayer(\n",
      "  (self_attn): GemmaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (mlp): GemmaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "    (act_fn): GELUActivation()\n",
      "  )\n",
      "  (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "), 'model.layers.15.self_attn': GemmaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "), 'model.layers.15.self_attn.q_proj': Linear(in_features=2048, out_features=2048, bias=False), 'model.layers.15.self_attn.k_proj': Linear(in_features=2048, out_features=256, bias=False), 'model.layers.15.self_attn.v_proj': Linear(in_features=2048, out_features=256, bias=False), 'model.layers.15.self_attn.o_proj': Linear(in_features=2048, out_features=2048, bias=False), 'model.layers.15.mlp': GemmaMLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "  (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "  (act_fn): GELUActivation()\n",
      "), 'model.layers.15.mlp.gate_proj': Linear(in_features=2048, out_features=16384, bias=False), 'model.layers.15.mlp.up_proj': Linear(in_features=2048, out_features=16384, bias=False), 'model.layers.15.mlp.down_proj': Linear(in_features=16384, out_features=2048, bias=False), 'model.layers.15.mlp.act_fn': GELUActivation(), 'model.layers.15.input_layernorm': GemmaRMSNorm((2048,), eps=1e-06), 'model.layers.15.post_attention_layernorm': GemmaRMSNorm((2048,), eps=1e-06), 'model.layers.16': GemmaDecoderLayer(\n",
      "  (self_attn): GemmaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (mlp): GemmaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "    (act_fn): GELUActivation()\n",
      "  )\n",
      "  (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "), 'model.layers.16.self_attn': GemmaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "), 'model.layers.16.self_attn.q_proj': Linear(in_features=2048, out_features=2048, bias=False), 'model.layers.16.self_attn.k_proj': Linear(in_features=2048, out_features=256, bias=False), 'model.layers.16.self_attn.v_proj': Linear(in_features=2048, out_features=256, bias=False), 'model.layers.16.self_attn.o_proj': Linear(in_features=2048, out_features=2048, bias=False), 'model.layers.16.mlp': GemmaMLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "  (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "  (act_fn): GELUActivation()\n",
      "), 'model.layers.16.mlp.gate_proj': Linear(in_features=2048, out_features=16384, bias=False), 'model.layers.16.mlp.up_proj': Linear(in_features=2048, out_features=16384, bias=False), 'model.layers.16.mlp.down_proj': Linear(in_features=16384, out_features=2048, bias=False), 'model.layers.16.mlp.act_fn': GELUActivation(), 'model.layers.16.input_layernorm': GemmaRMSNorm((2048,), eps=1e-06), 'model.layers.16.post_attention_layernorm': GemmaRMSNorm((2048,), eps=1e-06), 'model.layers.17': GemmaDecoderLayer(\n",
      "  (self_attn): GemmaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (mlp): GemmaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "    (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "    (act_fn): GELUActivation()\n",
      "  )\n",
      "  (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "  (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
      "), 'model.layers.17.self_attn': GemmaAttention(\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "  (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "  (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "), 'model.layers.17.self_attn.q_proj': Linear(in_features=2048, out_features=2048, bias=False), 'model.layers.17.self_attn.k_proj': Linear(in_features=2048, out_features=256, bias=False), 'model.layers.17.self_attn.v_proj': Linear(in_features=2048, out_features=256, bias=False), 'model.layers.17.self_attn.o_proj': Linear(in_features=2048, out_features=2048, bias=False), 'model.layers.17.mlp': GemmaMLP(\n",
      "  (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "  (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
      "  (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
      "  (act_fn): GELUActivation()\n",
      "), 'model.layers.17.mlp.gate_proj': Linear(in_features=2048, out_features=16384, bias=False), 'model.layers.17.mlp.up_proj': Linear(in_features=2048, out_features=16384, bias=False), 'model.layers.17.mlp.down_proj': Linear(in_features=16384, out_features=2048, bias=False), 'model.layers.17.mlp.act_fn': GELUActivation(), 'model.layers.17.input_layernorm': GemmaRMSNorm((2048,), eps=1e-06), 'model.layers.17.post_attention_layernorm': GemmaRMSNorm((2048,), eps=1e-06), 'model.norm': GemmaRMSNorm((2048,), eps=1e-06), 'model.rotary_emb': GemmaRotaryEmbedding(), 'lm_head': Linear(in_features=2048, out_features=256000, bias=False)}\n"
     ]
    }
   ],
   "execution_count": 23
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
