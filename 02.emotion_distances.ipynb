{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T20:30:06.426101Z",
     "start_time": "2025-08-12T20:30:06.423018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import load, nn, cdist\n",
    "from transformers import AutoModel, AutoTokenizer"
   ],
   "id": "519d59afd0a0d422",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T20:30:06.447159Z",
     "start_time": "2025-08-12T20:30:06.443909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MODEL = \"google/gemma-2b\"\n",
    "EMBEDDING_FILE = \"embeddings_google_gemma-2b.pth\"\n",
    "DEVICE = \"cuda:0\" # run on my gpu"
   ],
   "id": "74708e76fde1b8cb",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T20:30:07.479959Z",
     "start_time": "2025-08-12T20:30:06.451646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load pretrained tokenizer from model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ],
   "id": "1d14f8053e9d7d30",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T20:30:08.262428Z",
     "start_time": "2025-08-12T20:30:07.554480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load local embedding file\n",
    "saved_embeddings = load(EMBEDDING_FILE)\n",
    "if 'weight' not in saved_embeddings:\n",
    "    raise KeyError(\"The saved embeddings file does not contain 'weight' key.\")\n",
    "embeddings_tensor = saved_embeddings['weight']"
   ],
   "id": "54159fd0502cc305",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-12T20:30:11.256796Z",
     "start_time": "2025-08-12T20:30:08.267219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create an embedding only model object\n",
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(EmbeddingModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        return self.embedding(input_ids)\n",
    "\n",
    "# Create an empty LLM model that has the same shape with extracted embeddings\n",
    "emb_model = EmbeddingModel(*embeddings_tensor.size())\n",
    "\n",
    "# Give the LLM model the weight of extracted embeddings\n",
    "emb_model.embedding.weight.data = embeddings_tensor\n",
    "emb_model.eval()"
   ],
   "id": "f3b47355bf7948c8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbeddingModel(\n",
       "  (embedding): Embedding(256000, 2048)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T20:30:11.351539Z",
     "start_time": "2025-08-12T20:30:11.344946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def word_to_embeddings(word: str):\n",
    "    # tokenize\n",
    "    token_id = tokenizer(word, return_tensors=\"pt\")['input_ids']\n",
    "\n",
    "    # make a forward pass through custom model\n",
    "    embeddings = emb_model(token_id)\n",
    "\n",
    "    return embeddings"
   ],
   "id": "bd20eb0268dd58e6",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T20:30:11.384676Z",
     "start_time": "2025-08-12T20:30:11.355181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "emotions = [\"happy\", \"sad\", \"anxious\", \"calm\", \"depressed\", \"elated\"]\n",
    "emo_embeddings = {emo: word_to_embeddings(emo) for emo in emotions}"
   ],
   "id": "9431c8370825a4f0",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T23:49:08.077898Z",
     "start_time": "2025-08-12T23:49:08.066871Z"
    }
   },
   "cell_type": "code",
   "source": [
    "emotion_tokens = {emo: tokenizer(emo, return_tensors='pt').input_ids for emo in emotions}\n",
    "print(\"tokens: \", emotion_tokens)\n",
    "print(\"2: \", tokenizer.decode(2))\n",
    "print(\"3243: \", tokenizer.decode(3243))\n",
    "print(\"3734: \", tokenizer.decode(3734))\n",
    "print(\"After one forward pass through embedding layer.\")\n",
    "print(\"embeddings: \", {k: v.shape for k, v in emo_embeddings.items()})"
   ],
   "id": "f845862bfa6a8854",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens:  {'happy': tensor([[    2, 11896]]), 'sad': tensor([[    2, 37968]]), 'anxious': tensor([[    2,   481, 24192]]), 'calm': tensor([[     2, 116051]]), 'depressed': tensor([[   2, 3243, 3734]]), 'elated': tensor([[  2, 521, 840]])}\n",
      "2:  <bos>\n",
      "3243:  dep\n",
      "3734:  ressed\n",
      "After one forward pass through embedding layer.\n",
      "embeddings:  {'happy': torch.Size([1, 2, 2048]), 'sad': torch.Size([1, 2, 2048]), 'anxious': torch.Size([1, 3, 2048]), 'calm': torch.Size([1, 2, 2048]), 'depressed': torch.Size([1, 3, 2048]), 'elated': torch.Size([1, 3, 2048])}\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T21:31:21.799142Z",
     "start_time": "2025-08-12T21:31:21.779703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# cosine similarity measures angle between vectors (direction, not magnitude), it measures semantic similarity\n",
    "# https://www.learndatasci.com/glossary/cosine-similarity/\n",
    "# https://docs.pytorch.org/docs/stable/generated/torch.nn.CosineSimilarity.html\n",
    "# Dimension variable expected to be in range of [-3, 2]\n",
    "cos = nn.CosineSimilarity(dim=2)\n",
    "\n",
    "cos_dist = cos(emo_embeddings[\"happy\"], emo_embeddings[\"sad\"])\n",
    "print(cos_dist)\n",
    "print(cos_dist.shape)\n"
   ],
   "id": "e7f805d5ae8509fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.8699]], grad_fn=<SumBackward1>)\n",
      "torch.Size([1, 2])\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T21:31:23.338860Z",
     "start_time": "2025-08-12T21:31:23.320587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Euclidean for magnitude differences\n",
    "euclid_dist = cdist(emo_embeddings[\"happy\"], emo_embeddings[\"sad\"])\n",
    "print(euclid_dist)\n",
    "print(euclid_dist.shape)\n",
    "\n",
    "euclid_dist = cdist(emo_embeddings[\"happy\"], emo_embeddings[\"depressed\"])\n",
    "print(euclid_dist)\n",
    "print(euclid_dist.shape)\n"
   ],
   "id": "57c2ff7bc91af457",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0000, 15.3405],\n",
      "         [15.0511,  3.2664]]], grad_fn=<CdistBackward0>)\n",
      "torch.Size([1, 2, 2])\n",
      "tensor([[[ 0.0000, 15.0569, 15.0977],\n",
      "         [15.0511,  3.6696,  3.4826]]], grad_fn=<CdistBackward0>)\n",
      "torch.Size([1, 2, 3])\n"
     ]
    }
   ],
   "execution_count": 47
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
