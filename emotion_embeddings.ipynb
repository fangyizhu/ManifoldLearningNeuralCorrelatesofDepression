{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T23:39:38.482458Z",
     "start_time": "2025-08-11T23:39:27.906040Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import save, load, nn\n",
    "from transformers import AutoModel, AutoTokenizer"
   ],
   "id": "519d59afd0a0d422",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T23:39:38.494486Z",
     "start_time": "2025-08-11T23:39:38.491598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MODEL = \"google/gemma-2b\"\n",
    "EMBEDDING_FILE = \"embeddings_google_gemma-2b.pth\"\n",
    "DEVICE = \"cuda:0\" # run on my gpu"
   ],
   "id": "74708e76fde1b8cb",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T23:39:39.704171Z",
     "start_time": "2025-08-11T23:39:38.505751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load pretrained tokenizer from model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ],
   "id": "1d14f8053e9d7d30",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T23:39:44.981087Z",
     "start_time": "2025-08-11T23:39:39.775276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load local embedding file\n",
    "saved_embeddings = load(EMBEDDING_FILE)\n",
    "if 'weight' not in saved_embeddings:\n",
    "    raise KeyError(\"The saved embeddings file does not contain 'weight' key.\")\n",
    "embeddings_tensor = saved_embeddings['weight']"
   ],
   "id": "54159fd0502cc305",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-11T23:39:48.061248Z",
     "start_time": "2025-08-11T23:39:45.061424Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create an embedding only model object\n",
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(EmbeddingModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        return self.embedding(input_ids)\n",
    "\n",
    "# Create an LLM model that has the same shape with extracted embeddings\n",
    "emb_model = EmbeddingModel(*embeddings_tensor.size())\n",
    "emb_model.embedding.weight.data = embeddings_tensor\n",
    "emb_model.eval()"
   ],
   "id": "f3b47355bf7948c8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbeddingModel(\n",
       "  (embedding): Embedding(256000, 2048)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T23:39:48.154668Z",
     "start_time": "2025-08-11T23:39:48.150064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def word_to_embeddings(word: str):\n",
    "    # tokenize\n",
    "    token_id = tokenizer(word, return_tensors=\"pt\")['input_ids']\n",
    "\n",
    "    # make a forward pass through custom model\n",
    "    embeddings = emb_model(token_id)\n",
    "\n",
    "    return embeddings"
   ],
   "id": "bd20eb0268dd58e6",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T23:50:45.585264Z",
     "start_time": "2025-08-11T23:50:45.579955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "emotions = [\"happy\", \"sad\", \"anxious\", \"calm\", \"depressed\", \"elated\"]\n",
    "emo_embeddings = [word_to_embeddings(emo) for emo in emotions]"
   ],
   "id": "9431c8370825a4f0",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def cosine_sim(word1: str, word2: str):\n",
    "    # cosine similarity measures angle between vectors (direction, not magnitude), it measures semantic similarity\n",
    "    # https://www.learndatasci.com/glossary/cosine-similarity/\n",
    "\n"
   ],
   "id": "e7f805d5ae8509fe"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
